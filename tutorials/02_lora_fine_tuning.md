# LoRA Fine-Tuning Workflow

This tutorial expands on `train_scientific_code_model.py` and shows how to adapt it for your own datasets and hyperparameters. By the end you will have a repeatable pipeline for fine-tuning a Tinker-supported base model with Low Rank Adaptation (LoRA).

---

## 1. Why LoRA?

LoRA injects small, trainable rank-decomposition matrices into a frozen base model. The benefits:

- Reduces GPU memory requirements for training
- Keeps the original inference stack intact
- Lets you swap LoRA adapters in and out for different tasks

Tinker exposes this through the `create_lora_training_client` helper on the service client.

---

## 2. Script Anatomy

Open `train_scientific_code_model.py` and locate the main steps:

1. Validate the `TINKER_API_KEY`
2. Create a `ServiceClient` and list models
3. Build a LoRA training client
4. Load JSON training examples
5. Convert examples into `types.Datum` objects
6. Run the training loop (forward-backward plus optimizer steps)
7. Save the adapter weights and obtain a sampling client

Each step is annotated in the script, so skim through to map code to this list.

---

## 3. Customizing Hyperparameters

You can experiment through command-line flags:

```bash
python train_scientific_code_model.py \
  --data data/my_examples.json \
  --model Qwen/Qwen3-30B-A3B-Base \
  --epochs 5 \
  --lr 5e-5 \
  --name my-scientific-helper
```

### Tips
- Use a smaller learning rate (`1e-5` to `5e-5`) when training data is noisy.
- Start with fewer epochs and gradually increase once the loss decreases steadily.
- Rename the adapter (`--name`) per experiment to avoid overwriting outputs.

---

## 4. Adding New Training Examples

The default dataset is generated by `generate_training_data.py`. To extend it:

1. Clone an existing example structure (task plus code).
2. Keep prompts concise and focused on the desired code pattern.
3. Add variations that stress edge cases or uncommon libraries.
4. Regenerate the JSON file by running the script or writing your own pipeline that dumps the same schema.

Make sure the resulting JSON lives under the `data/` directory so Git tracks it and the training script finds it.

---

## 5. Inspecting the Tokenized Samples

`visualize_example` renders the first few tokens, weights, and decoded strings. Call it on additional examples if you need to debug formatting issues:

```python
visualize_example(processed_examples[5], tokenizer, max_tokens=40)
```

Look for:
- Input tokens with weight 0 (prompt) and output tokens with weight 1 (code)
- Proper line breaks, especially before code blocks
- Balanced parentheses and indentation

---

## 6. Tracking Loss Across Epochs

The script prints a single loss value per epoch by combining log probabilities and weights. For richer logging:

- Store `loss` in a list and plot it with Matplotlib
- Emit metrics to a CSV or JSON file
- Integrate with your observability stack (Weights and Biases, MLflow, and so on)

```python
loss_history = []
...
loss_history.append(loss)
```

After training finishes, save the values and visualize them later.

---

## 7. Testing the Adapter

Immediately after saving weights, the script samples the fine-tuned model with a held-out task. Modify `test_task` to cover scenarios that matter to you, or call the returned `sampling_client` from another module or notebook.

Remember to keep adapter files organized. If you download them locally, store them alongside experiment metadata so you can reproduce runs later.

---

## 8. Next Steps

- Evaluate with the [Evaluation and Monitoring guide](03_evaluation_and_monitoring.md).
- Try curriculum learning by training on progressively harder examples.
- Schedule multiple runs using different base models and compare losses.
